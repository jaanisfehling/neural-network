Custom neural network implemented using numpy.
Architecture: LinearLayer(12, 64), ReLU, LinearLayer(64, 64), ReLU, LinearLayer(64, 64), ReLU, LinearLayer(64, 4), 
Trained using MSE Loss and AdamW optimizer with learning rate 0.01 and weight decay 0.005.
Grid Search + Cross Validation and holding out the test set.
Batch size: 32, early stopping with patience 5, trained in 29 Epochs.