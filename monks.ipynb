{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26978745",
   "metadata": {},
   "source": [
    "# Monk's Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0e8fe",
   "metadata": {},
   "source": [
    "Import our own neural network code aswell as numpy and pandas for data handling and loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354ea9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from network import *\n",
    "from train_utils import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f442d1",
   "metadata": {},
   "source": [
    "Read the data into pandas dataframes, remove empty first column, remove ids, split into variables and class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33fce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 6)\n",
      "   2  3  4  5  6  7\n",
      "0  1  1  1  1  3  1\n",
      "1  1  1  1  1  3  2\n",
      "2  1  1  1  3  2  1\n",
      "3  1  1  1  3  3  2\n",
      "4  1  1  2  1  2  1\n",
      "(124,)\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X1_df = pd.read_csv(\"data/monks/monks-1.train\", sep=\" \", header=None)\n",
    "XT1_df = pd.read_csv(\"data/monks/monks-1.test\", sep=\" \", header=None)\n",
    "X2_df = pd.read_csv(\"data/monks/monks-2.train\", sep=\" \", header=None)\n",
    "XT2_df = pd.read_csv(\"data/monks/monks-2.test\", sep=\" \", header=None)\n",
    "X3_df = pd.read_csv(\"data/monks/monks-3.train\", sep=\" \", header=None)\n",
    "XT3_df = pd.read_csv(\"data/monks/monks-3.test\", sep=\" \", header=None)\n",
    "\n",
    "Y1_df = X1_df.pop(1)\n",
    "X1_df = X1_df.drop(columns=[0, 8])\n",
    "YT1_df = XT1_df.pop(1)\n",
    "XT1_df = XT1_df.drop(columns=[0, 8])\n",
    "Y2_df = X2_df.pop(1)\n",
    "X2_df = X2_df.drop(columns=[0, 8])\n",
    "YT2_df = XT2_df.pop(1)\n",
    "XT2_df = XT2_df.drop(columns=[0, 8])\n",
    "Y3_df = X3_df.pop(1)\n",
    "X3_df = X3_df.drop(columns=[0, 8])\n",
    "YT3_df = XT3_df.pop(1)\n",
    "XT3_df = XT3_df.drop(columns=[0, 8])\n",
    "\n",
    "print(X1_df.shape)\n",
    "print(X1_df.head())\n",
    "print(Y1_df.shape)\n",
    "print(Y1_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db301aef",
   "metadata": {},
   "source": [
    "Apply one-hot encoding to each variable and convert to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb991ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 17)\n",
      "(124, 1)\n",
      "[[1 0 0 ... 0 1 0]\n",
      " [1 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 1]\n",
      " [0 0 1 ... 0 0 1]\n",
      " [0 0 1 ... 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_feature(df, feature_to_encode):\n",
    "    dummies = pd.get_dummies(df[feature_to_encode], dtype=\"int32\", prefix=feature_to_encode)\n",
    "    result_df = pd.concat([df, dummies], axis=1)\n",
    "    return result_df.drop(columns=feature_to_encode)\n",
    "\n",
    "for feature in X1_df.columns:\n",
    "    X1_df = one_hot_encode_feature(X1_df, feature)\n",
    "    XT1_df = one_hot_encode_feature(XT1_df, feature)\n",
    "    X2_df = one_hot_encode_feature(X2_df, feature)\n",
    "    XT2_df = one_hot_encode_feature(XT2_df, feature)\n",
    "    X3_df = one_hot_encode_feature(X3_df, feature)\n",
    "    XT3_df = one_hot_encode_feature(XT3_df, feature)\n",
    "\n",
    "X1 = X1_df.to_numpy()\n",
    "Y1 = Y1_df.to_numpy().reshape(-1, 1)\n",
    "XT1 = XT1_df.to_numpy()\n",
    "YT1 = YT1_df.to_numpy().reshape(-1, 1)\n",
    "X2 = X2_df.to_numpy()\n",
    "Y2 = Y2_df.to_numpy().reshape(-1, 1)\n",
    "XT2 = XT2_df.to_numpy()\n",
    "YT2 = YT2_df.to_numpy().reshape(-1, 1)\n",
    "X3 = X3_df.to_numpy()\n",
    "Y3 = Y3_df.to_numpy().reshape(-1, 1)\n",
    "XT3 = XT3_df.to_numpy()\n",
    "YT3 = YT3_df.to_numpy().reshape(-1, 1)\n",
    "print(X1.shape)\n",
    "print(Y1.shape)\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6f2cf",
   "metadata": {},
   "source": [
    "## Initial Training\n",
    "We first train with hould-out validation and hold-out test to see if training works at all.\n",
    "\n",
    "Split data into train and validation sets, using 80% of the data for train and 20% for validation. The test set is already given as part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd80646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 17) (99, 1) (25, 17) (25, 1)\n"
     ]
    }
   ],
   "source": [
    "def split_data(X, Y, train_fraction=0.8, shuffle=True):\n",
    "    n = X.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_size = int(n * train_fraction)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    Y_test = Y[test_indices]\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "XTr1, YTr1, XVl1, YVl1 = split_data(X1, Y1)\n",
    "XTr2, YTr2, XVl2, YVl2 = split_data(X2, Y2)\n",
    "XTr3, YTr3, XVl3, YVl3 = split_data(X3, Y3)\n",
    "\n",
    "print(XTr1.shape, YTr1.shape, XVl1.shape, YVl1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7eae9f",
   "metadata": {},
   "source": [
    "Define datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e163cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTr1_dl = DataLoader(Dataset(XTr1, YTr1), batch_size=8, shuffle=True)\n",
    "XTr2_dl = DataLoader(Dataset(XTr2, YTr2), batch_size=8, shuffle=True)\n",
    "XTr3_dl = DataLoader(Dataset(XTr3, YTr3), batch_size=8, shuffle=True)\n",
    "XVl1_dl = DataLoader(Dataset(XVl1, YVl1), batch_size=8, shuffle=False)\n",
    "XVl2_dl = DataLoader(Dataset(XVl2, YVl2), batch_size=8, shuffle=False)\n",
    "XVl3_dl = DataLoader(Dataset(XVl3, YVl3), batch_size=8, shuffle=False)\n",
    "XT1_dl = DataLoader(Dataset(XT1, YT1), batch_size=8, shuffle=False)\n",
    "XT2_dl = DataLoader(Dataset(XT2, YT2), batch_size=8, shuffle=False)\n",
    "XT3_dl = DataLoader(Dataset(XT3, YT3), batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d6868",
   "metadata": {},
   "source": [
    "Train a small model to see if training works at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60aca017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 0.7875 | Train Acc: 0.4343 | Val Loss: 0.7253 | Val Acc: 0.3600\n",
      "Epoch 002 | Train Loss: 0.6836 | Train Acc: 0.5253 | Val Loss: 0.7049 | Val Acc: 0.4400\n",
      "Epoch 003 | Train Loss: 0.6119 | Train Acc: 0.6162 | Val Loss: 0.6910 | Val Acc: 0.3600\n",
      "Epoch 004 | Train Loss: 0.5630 | Train Acc: 0.6465 | Val Loss: 0.6638 | Val Acc: 0.4800\n",
      "Epoch 005 | Train Loss: 0.5213 | Train Acc: 0.6465 | Val Loss: 0.6293 | Val Acc: 0.5200\n",
      "Epoch 006 | Train Loss: 0.4788 | Train Acc: 0.7374 | Val Loss: 0.6039 | Val Acc: 0.7600\n",
      "Epoch 007 | Train Loss: 0.4500 | Train Acc: 0.8081 | Val Loss: 0.5906 | Val Acc: 0.7200\n",
      "Epoch 008 | Train Loss: 0.4220 | Train Acc: 0.8384 | Val Loss: 0.5780 | Val Acc: 0.7200\n",
      "Epoch 009 | Train Loss: 0.3904 | Train Acc: 0.8384 | Val Loss: 0.5733 | Val Acc: 0.7200\n",
      "Epoch 010 | Train Loss: 0.3655 | Train Acc: 0.8586 | Val Loss: 0.5771 | Val Acc: 0.7200\n",
      "Epoch 011 | Train Loss: 0.3467 | Train Acc: 0.8586 | Val Loss: 0.5768 | Val Acc: 0.7200\n",
      "Epoch 012 | Train Loss: 0.3384 | Train Acc: 0.8788 | Val Loss: 0.5931 | Val Acc: 0.7200\n",
      "Epoch 013 | Train Loss: 0.3156 | Train Acc: 0.9091 | Val Loss: 0.5850 | Val Acc: 0.7200\n",
      "Epoch 014 | Train Loss: 0.3092 | Train Acc: 0.9091 | Val Loss: 0.5959 | Val Acc: 0.7200\n",
      "Epoch 015 | Train Loss: 0.2923 | Train Acc: 0.9091 | Val Loss: 0.5953 | Val Acc: 0.7200\n",
      "Epoch 016 | Train Loss: 0.2867 | Train Acc: 0.9091 | Val Loss: 0.6050 | Val Acc: 0.6800\n",
      "Epoch 017 | Train Loss: 0.2773 | Train Acc: 0.9091 | Val Loss: 0.6195 | Val Acc: 0.6800\n",
      "Epoch 018 | Train Loss: 0.2671 | Train Acc: 0.9091 | Val Loss: 0.6149 | Val Acc: 0.7200\n",
      "Epoch 019 | Train Loss: 0.2585 | Train Acc: 0.9293 | Val Loss: 0.6105 | Val Acc: 0.7200\n",
      "Epoch 020 | Train Loss: 0.2529 | Train Acc: 0.9192 | Val Loss: 0.6306 | Val Acc: 0.7200\n",
      "Epoch 021 | Train Loss: 0.2432 | Train Acc: 0.9192 | Val Loss: 0.6271 | Val Acc: 0.7200\n",
      "Epoch 022 | Train Loss: 0.2346 | Train Acc: 0.9192 | Val Loss: 0.6366 | Val Acc: 0.7200\n",
      "Epoch 023 | Train Loss: 0.2289 | Train Acc: 0.9293 | Val Loss: 0.6426 | Val Acc: 0.7200\n",
      "Epoch 024 | Train Loss: 0.2205 | Train Acc: 0.9394 | Val Loss: 0.6470 | Val Acc: 0.7200\n",
      "Epoch 025 | Train Loss: 0.2125 | Train Acc: 0.9293 | Val Loss: 0.6288 | Val Acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    LinearLayer(17, 3),\n",
    "    ReLU(),\n",
    "    LinearLayer(3, 1),\n",
    "    Sigmoid(),\n",
    ")\n",
    "loss_fn = BCELoss()\n",
    "optimizer = AdamWOptimizer(model, learning_rate=0.01, weight_decay=0.01)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "    # Train\n",
    "    train_total_n = 0\n",
    "    train_losses = 0.0\n",
    "    train_correct_n = 0\n",
    "    for x_batch, y_batch in XTr1_dl:\n",
    "        y_pred = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(y_pred, y_batch)\n",
    "        grad_loss = loss_fn.backward()\n",
    "        model.backward(grad_loss)\n",
    "        optimizer.step()\n",
    "        train_total_n += y_batch.shape[0]\n",
    "        train_losses += loss * y_batch.shape[0]\n",
    "        y_hat = (y_pred >= 0.5).astype(int)\n",
    "        train_correct_n += np.sum(y_hat == y_batch)\n",
    "\n",
    "    # Validate\n",
    "    val_total_n = 0\n",
    "    val_losses = 0.0\n",
    "    val_correct_n = 0\n",
    "    for x_batch, y_batch in XVl1_dl:\n",
    "        y_pred = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(y_pred, y_batch)\n",
    "        val_total_n += y_batch.shape[0]\n",
    "        val_losses += loss * y_batch.shape[0]\n",
    "        y_hat = (y_pred >= 0.5).astype(int)\n",
    "        val_correct_n += np.sum(y_hat == y_batch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d} | Train Loss: {train_losses / train_total_n:.4f} | Train Acc: {train_correct_n / train_total_n:.4f} | Val Loss: {val_losses / val_total_n:.4f} | Val Acc: {val_correct_n / val_total_n:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "03709e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4795 | Test Acc: 0.7824\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "total_n = 0\n",
    "test_losses = 0.0\n",
    "correct_n = 0\n",
    "for x_batch, y_batch in XT1_dl:\n",
    "    y_pred = model.forward(x_batch)\n",
    "    loss = loss_fn.forward(y_pred, y_batch)\n",
    "    total_n += y_batch.shape[0]\n",
    "    test_losses += loss * y_batch.shape[0]\n",
    "    y_hat = (y_pred >= 0.5).astype(int)\n",
    "    correct_n += np.sum(y_hat == y_batch)\n",
    "\n",
    "print(f\"Test Loss: {test_losses / total_n:.4f} | Test Acc: {correct_n / total_n:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971fa83",
   "metadata": {},
   "source": [
    "## Cross Validation + Grid Search\n",
    "Now that we have a basic model that is able to learn, we do a cross validation with grid search and hold out the test set each time (because the test set was already separated as part of the Monk's problem).\n",
    "\n",
    "The relevant functions are in `train_utils.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2268\n",
      "{'learning_rate': 0.0005, 'weight_decay': 0.0005, 'input_size': 17, 'hidden_units_layer_1': 2, 'hidden_units_layer_2': 0, 'hidden_units_layer_3': 0, 'batch_size': 8, 'output_size': 1, 'activation_hidden_layer': <class 'network.relu.ReLU'>, 'activation_output_layer': <class 'network.sigmoid.Sigmoid'>, 'loss_function': <class 'network.bce.BCELoss'>, 'optimizer': <class 'network.adam.AdamWOptimizer'>}\n"
     ]
    }
   ],
   "source": [
    "parameters = dict(\n",
    "    learning_rates = [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    weight_decays = [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    input_size = 17,\n",
    "    hidden_units_layer_1 = [2, 3, 4, 5, 6, 7],\n",
    "    hidden_units_layer_2 = [0, 2, 3, 4, 5, 6, 7],\n",
    "    hidden_units_layer_3 = [0],\n",
    "    output_size = 1,\n",
    "    batch_sizes = [8],\n",
    "    activations_hidden_layer = [ReLU],\n",
    "    activations_output_layer = [Sigmoid],\n",
    "    loss_functions = [BCELoss],\n",
    "    optimizers = [AdamWOptimizer],\n",
    ")\n",
    "\n",
    "cfgs = generate_param_cfgs(parameters)\n",
    "print(len(cfgs))\n",
    "print(cfgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a7180",
   "metadata": {},
   "source": [
    "First Monk 1, then 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe28837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2268/2268 [06:57<00:00,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'learning_rate': 0.05, 'weight_decay': 0.1, 'input_size': 17, 'hidden_units_layer_1': 6, 'hidden_units_layer_2': 6, 'hidden_units_layer_3': 0, 'batch_size': 8, 'output_size': 1, 'activation_hidden_layer': <class 'network.relu.ReLU'>, 'activation_output_layer': <class 'network.sigmoid.Sigmoid'>, 'loss_function': <class 'network.bce.BCELoss'>, 'optimizer': <class 'network.adam.AdamWOptimizer'>}\n",
      "Best validation acc: 0.8030\n",
      "Used epochs: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = grid_search_cross_validate(X1, Y1, cfgs, final_metric_fn=accuracy)\n",
    "best_cfg, best_result, epochs = max(results, key=lambda x: x[1])\n",
    "print(\"Best config:\", best_cfg)\n",
    "print(f\"Best validation acc: {best_result:.4f}\")\n",
    "print(f\"Used epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4fd4b2",
   "metadata": {},
   "source": [
    "Retrain best found parameters on train + validation set, then assess performance on test set. Repeat n times and average results, because dataset is very small and initialization matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9624c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4262 | Test Acc: 0.7733\n"
     ]
    }
   ],
   "source": [
    "X1_dl = DataLoader(Dataset(X1, Y1), batch_size=best_cfg['batch_size'], shuffle=False)\n",
    "\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for _ in range(100):\n",
    "    best_model = create_model(best_cfg)\n",
    "    loss_fn = best_cfg['loss_function']()\n",
    "    optimizer = best_cfg['optimizer'](best_model, learning_rate=best_cfg['learning_rate'], weight_decay=best_cfg['weight_decay'])\n",
    "    best_model, _, _ = train_model(best_model, loss_fn, optimizer, X1_dl, None, accuracy, max_epochs=epochs)\n",
    "    test_loss, test_acc = epoch_run(best_model, loss_fn, None, XT1_dl, accuracy, update_weights=False)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "test_loss = np.mean(test_losses)\n",
    "test_acc = np.mean(test_accs)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457db30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
