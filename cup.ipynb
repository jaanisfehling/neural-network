{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0054bf",
   "metadata": {},
   "source": [
    "# ML Cup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027379af",
   "metadata": {},
   "source": [
    "Import our own neural network code aswell as numpy and pandas for data handling and loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "0bd323ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from network import *\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99152cf",
   "metadata": {},
   "source": [
    "Read the data into pandas dataframes, remove empty first column, remove ids, split into variables and class labels, convert to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "611db014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 12)\n",
      "          1          2          3          4         5         6          7   \\\n",
      "0  -6.925642  -6.093158  -9.149763  -5.918488  4.391259 -1.059304  -5.031085   \n",
      "1  -5.649870  -7.650998 -10.407383  -7.864047  3.790306 -1.673732  -8.493233   \n",
      "2  15.985886  14.192953  24.466835  12.551305 -7.788409  0.557977  23.145951   \n",
      "3  12.774004  10.156462  18.588934   8.346695 -5.245173 -0.199274  14.500231   \n",
      "4  -4.019226  -4.043457  -5.095354  -3.147125  0.725466 -0.477673  -4.025913   \n",
      "\n",
      "          8          9          10         11         12  \n",
      "0  -6.932177  -5.805652   7.147028   4.555533  -5.694865  \n",
      "1  -8.143588  -9.447557  10.790796   6.266211  -5.551301  \n",
      "2  20.031774  14.516358 -21.024198 -10.410913  12.061133  \n",
      "3  12.608063  12.411055 -15.479452  -8.871887   6.703585  \n",
      "4  -0.995364  -3.491760   3.385533   1.838361  -4.271710  \n",
      "(500, 4)\n",
      "          13         14         15         16\n",
      "0   6.554997  10.688732  15.416160  -7.535628\n",
      "1  12.342252  -8.135250  23.787661  -3.270978\n",
      "2  28.542661 -14.132383 -56.408372   1.892238\n",
      "3  20.253500   9.525402  -0.673842  40.295464\n",
      "4  -3.588910   6.050010   1.198489 -11.677909\n",
      "(1000, 12)\n",
      "          1          2          3          4         5         6          7   \\\n",
      "0  -1.948207  -4.831121  -8.003946  -3.268500  2.222619 -1.595923  -5.323413   \n",
      "1  14.990087  13.501754  25.691005  11.910876 -8.300274  0.364864  23.035523   \n",
      "2   2.893488   4.681421   7.148577   3.731728 -1.834960 -2.965363   5.521146   \n",
      "3  11.892590   9.223032  13.903191   7.440720 -6.288820  0.694170  17.971045   \n",
      "4  13.459402  12.502286  23.704447  12.784438 -8.024671 -0.279835  17.446072   \n",
      "\n",
      "          8          9          10        11         12  \n",
      "0  -3.541106  -2.668865   3.108137  3.501315  -2.778303  \n",
      "1  17.730048  12.980467 -19.740962 -8.864486  10.315840  \n",
      "2   5.255667   1.841639  -2.679762 -1.766710   2.563493  \n",
      "3  13.520892  11.306512 -12.436463 -5.639603   6.603149  \n",
      "4  13.328186  12.805979 -14.407507 -8.312281  10.776876  \n"
     ]
    }
   ],
   "source": [
    "X_df = pd.read_csv(\"data/cup/ML-CUP25-TR.csv\", sep=\",\", comment=\"#\", header=None)\n",
    "XBT_df = pd.read_csv(\"data/cup/ML-CUP25-TS.csv\", sep=\",\", comment=\"#\", header=None)\n",
    "\n",
    "Y_df = X_df[[13, 14, 15, 16]]\n",
    "X_df = X_df.drop(columns=[0, 13, 14, 15, 16])\n",
    "XBT_df = XBT_df.drop(columns=[0])\n",
    "\n",
    "print(X_df.shape)\n",
    "print(X_df.head())\n",
    "print(Y_df.shape)\n",
    "print(Y_df.head())\n",
    "print(XBT_df.shape)\n",
    "print(XBT_df.head())\n",
    "\n",
    "X = X_df.to_numpy()\n",
    "Y = Y_df.to_numpy()\n",
    "XBT = XBT_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cfae0",
   "metadata": {},
   "source": [
    "Scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "9c2c36f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.88733074, -0.81159466, -0.73626082, ...,  0.73052709,\n",
       "         0.83883143, -0.92278791],\n",
       "       [-0.76318039, -0.96810341, -0.81269752, ...,  1.02119652,\n",
       "         1.09256129, -0.9028031 ],\n",
       "       [ 1.34227906,  1.22645517,  1.30691755, ..., -1.51673883,\n",
       "        -1.38101066,  1.5489333 ],\n",
       "       ...,\n",
       "       [ 1.0711801 ,  1.07026272,  1.42800213, ..., -1.22023228,\n",
       "        -1.42278381,  1.2695007 ],\n",
       "       [-0.90892053, -1.26616062, -1.25202302, ...,  1.05068417,\n",
       "         0.99456614, -1.03309211],\n",
       "       [ 0.93705775,  0.62131415,  0.79163565, ..., -0.63697812,\n",
       "        -0.57424952,  0.53928755]], shape=(500, 12))"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = StandardScaler().fit_transform(X)\n",
    "XBT = StandardScaler().fit_transform(XBT)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663018a",
   "metadata": {},
   "source": [
    "## Initial Training\n",
    "We first train with hould-out validation and hold-out test to see if training works at all.\n",
    "\n",
    "Split data into train and validation sets, using 80% of the data for train and 20% for validation. The test set is already given as part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "3fbd82c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 12) (350, 4) (75, 12) (75, 4) (75, 12) (75, 4)\n"
     ]
    }
   ],
   "source": [
    "def split_data(X, Y, val_fraction=0.15, test_fraction=0.15, shuffle=True):\n",
    "    n = X.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    val_size = int(n * val_fraction)\n",
    "    test_size = int(n * test_fraction)\n",
    "    train_size = n - val_size - test_size\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    X_val = X[val_indices]\n",
    "    Y_val = Y[val_indices]\n",
    "    X_test = X[test_indices]\n",
    "    Y_test = Y[test_indices]\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "\n",
    "XTr, YTr, XVl, YVl, XT, YT = split_data(X, Y)\n",
    "print(XTr.shape, YTr.shape, XVl.shape, YVl.shape, XT.shape, YT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0a211",
   "metadata": {},
   "source": [
    "Define datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "3fe16596",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTr_dl = DataLoader(Dataset(XTr, YTr), batch_size=32, shuffle=True)\n",
    "XVl_dl = DataLoader(Dataset(XVl, YVl), batch_size=32, shuffle=False)\n",
    "XT_dl = DataLoader(Dataset(XT, YT), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971aa87",
   "metadata": {},
   "source": [
    "Train a small model to see if training works at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "47050cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 350.9647 | Train MEE: 17.3335 | Val Loss: 209.4403 | Val MEE: 13.0891\n",
      "Epoch 002 | Train Loss: 227.7675 | Train MEE: 13.8373 | Val Loss: 172.5407 | Val MEE: 12.0164\n",
      "Epoch 003 | Train Loss: 211.7595 | Train MEE: 13.3730 | Val Loss: 165.5326 | Val MEE: 11.6088\n",
      "Epoch 004 | Train Loss: 206.7572 | Train MEE: 13.1753 | Val Loss: 157.7312 | Val MEE: 11.3426\n",
      "Epoch 005 | Train Loss: 202.4942 | Train MEE: 13.0497 | Val Loss: 152.0193 | Val MEE: 11.2342\n",
      "Epoch 006 | Train Loss: 200.1930 | Train MEE: 12.9602 | Val Loss: 159.9800 | Val MEE: 11.3356\n",
      "Epoch 007 | Train Loss: 197.5500 | Train MEE: 12.8593 | Val Loss: 147.9509 | Val MEE: 11.0015\n",
      "Epoch 008 | Train Loss: 194.6747 | Train MEE: 12.6978 | Val Loss: 152.6846 | Val MEE: 11.0362\n",
      "Epoch 009 | Train Loss: 192.2584 | Train MEE: 12.6153 | Val Loss: 144.8592 | Val MEE: 10.8118\n",
      "Epoch 010 | Train Loss: 192.0305 | Train MEE: 12.5755 | Val Loss: 146.7057 | Val MEE: 10.8062\n",
      "Epoch 011 | Train Loss: 189.8349 | Train MEE: 12.4718 | Val Loss: 153.1712 | Val MEE: 10.9595\n",
      "Epoch 012 | Train Loss: 192.9663 | Train MEE: 12.5562 | Val Loss: 141.3779 | Val MEE: 10.6250\n",
      "Epoch 013 | Train Loss: 187.5126 | Train MEE: 12.4031 | Val Loss: 150.3119 | Val MEE: 10.8767\n",
      "Epoch 014 | Train Loss: 187.0129 | Train MEE: 12.3933 | Val Loss: 144.8809 | Val MEE: 10.6952\n",
      "Epoch 015 | Train Loss: 184.6379 | Train MEE: 12.3021 | Val Loss: 141.7834 | Val MEE: 10.6117\n",
      "Epoch 016 | Train Loss: 184.9559 | Train MEE: 12.3150 | Val Loss: 142.1633 | Val MEE: 10.5901\n",
      "Epoch 017 | Train Loss: 183.2853 | Train MEE: 12.2364 | Val Loss: 143.2369 | Val MEE: 10.6186\n",
      "Epoch 018 | Train Loss: 182.8389 | Train MEE: 12.2383 | Val Loss: 142.9663 | Val MEE: 10.6058\n",
      "Epoch 019 | Train Loss: 182.3005 | Train MEE: 12.2348 | Val Loss: 140.7976 | Val MEE: 10.5318\n",
      "Epoch 020 | Train Loss: 180.0135 | Train MEE: 12.1339 | Val Loss: 143.2221 | Val MEE: 10.5997\n",
      "Epoch 021 | Train Loss: 181.4531 | Train MEE: 12.1891 | Val Loss: 143.0741 | Val MEE: 10.5978\n",
      "Epoch 022 | Train Loss: 178.1844 | Train MEE: 12.0925 | Val Loss: 138.5769 | Val MEE: 10.4743\n",
      "Epoch 023 | Train Loss: 178.1520 | Train MEE: 12.0751 | Val Loss: 140.9417 | Val MEE: 10.5163\n",
      "Epoch 024 | Train Loss: 179.0961 | Train MEE: 12.1087 | Val Loss: 142.7018 | Val MEE: 10.5589\n",
      "Epoch 025 | Train Loss: 177.3179 | Train MEE: 12.0463 | Val Loss: 140.2867 | Val MEE: 10.5071\n",
      "Epoch 026 | Train Loss: 176.6818 | Train MEE: 11.9978 | Val Loss: 139.0429 | Val MEE: 10.4425\n",
      "Epoch 027 | Train Loss: 172.9570 | Train MEE: 11.8885 | Val Loss: 138.7036 | Val MEE: 10.4395\n",
      "Epoch 028 | Train Loss: 173.0135 | Train MEE: 11.8825 | Val Loss: 136.0564 | Val MEE: 10.3631\n",
      "Epoch 029 | Train Loss: 171.3915 | Train MEE: 11.8658 | Val Loss: 141.7143 | Val MEE: 10.4924\n",
      "Epoch 030 | Train Loss: 170.6407 | Train MEE: 11.8126 | Val Loss: 141.4103 | Val MEE: 10.5313\n",
      "Epoch 031 | Train Loss: 168.8222 | Train MEE: 11.7534 | Val Loss: 138.4459 | Val MEE: 10.4030\n",
      "Epoch 032 | Train Loss: 166.6505 | Train MEE: 11.6845 | Val Loss: 136.8126 | Val MEE: 10.3845\n",
      "Epoch 033 | Train Loss: 166.8124 | Train MEE: 11.6567 | Val Loss: 134.0648 | Val MEE: 10.2845\n",
      "Epoch 034 | Train Loss: 163.1553 | Train MEE: 11.5685 | Val Loss: 136.4492 | Val MEE: 10.3853\n",
      "Epoch 035 | Train Loss: 162.5518 | Train MEE: 11.5530 | Val Loss: 134.8742 | Val MEE: 10.2994\n",
      "Epoch 036 | Train Loss: 162.1796 | Train MEE: 11.5203 | Val Loss: 130.6169 | Val MEE: 10.1582\n",
      "Epoch 037 | Train Loss: 160.5177 | Train MEE: 11.4635 | Val Loss: 131.8112 | Val MEE: 10.1837\n",
      "Epoch 038 | Train Loss: 159.4128 | Train MEE: 11.3780 | Val Loss: 128.1525 | Val MEE: 10.0729\n",
      "Epoch 039 | Train Loss: 156.8415 | Train MEE: 11.3268 | Val Loss: 138.1950 | Val MEE: 10.3750\n",
      "Epoch 040 | Train Loss: 159.6639 | Train MEE: 11.4155 | Val Loss: 128.0055 | Val MEE: 10.0221\n",
      "Epoch 041 | Train Loss: 160.2580 | Train MEE: 11.4350 | Val Loss: 131.7208 | Val MEE: 10.1632\n",
      "Epoch 042 | Train Loss: 156.6210 | Train MEE: 11.3310 | Val Loss: 134.5005 | Val MEE: 10.2157\n",
      "Epoch 043 | Train Loss: 154.2823 | Train MEE: 11.2037 | Val Loss: 137.5936 | Val MEE: 10.2779\n",
      "Epoch 044 | Train Loss: 148.1394 | Train MEE: 11.0161 | Val Loss: 125.1905 | Val MEE: 9.9515\n",
      "Epoch 045 | Train Loss: 149.6648 | Train MEE: 11.0644 | Val Loss: 150.0740 | Val MEE: 10.6126\n",
      "Epoch 046 | Train Loss: 155.3957 | Train MEE: 11.2534 | Val Loss: 121.2729 | Val MEE: 9.8261\n",
      "Epoch 047 | Train Loss: 150.2146 | Train MEE: 11.1055 | Val Loss: 130.1845 | Val MEE: 10.0388\n",
      "Epoch 048 | Train Loss: 146.3482 | Train MEE: 10.9628 | Val Loss: 125.3118 | Val MEE: 9.9084\n",
      "Epoch 049 | Train Loss: 143.9490 | Train MEE: 10.8468 | Val Loss: 125.9324 | Val MEE: 9.8704\n",
      "Epoch 050 | Train Loss: 146.7394 | Train MEE: 10.9275 | Val Loss: 128.8555 | Val MEE: 10.0288\n",
      "Epoch 051 | Train Loss: 142.1613 | Train MEE: 10.7911 | Val Loss: 128.2547 | Val MEE: 9.9819\n",
      "Epoch 052 | Train Loss: 140.9033 | Train MEE: 10.7369 | Val Loss: 136.4136 | Val MEE: 10.1763\n",
      "Epoch 053 | Train Loss: 151.7107 | Train MEE: 11.0148 | Val Loss: 127.1685 | Val MEE: 9.9911\n",
      "Epoch 054 | Train Loss: 144.2388 | Train MEE: 10.8291 | Val Loss: 126.9828 | Val MEE: 9.8887\n",
      "Epoch 055 | Train Loss: 140.1431 | Train MEE: 10.6647 | Val Loss: 131.5715 | Val MEE: 10.0797\n",
      "Epoch 056 | Train Loss: 139.1814 | Train MEE: 10.6552 | Val Loss: 123.6321 | Val MEE: 9.7289\n",
      "Epoch 057 | Train Loss: 140.9069 | Train MEE: 10.6828 | Val Loss: 125.4454 | Val MEE: 9.8187\n",
      "Epoch 058 | Train Loss: 136.2048 | Train MEE: 10.5499 | Val Loss: 128.2325 | Val MEE: 9.9322\n",
      "Epoch 059 | Train Loss: 136.1032 | Train MEE: 10.4986 | Val Loss: 126.1571 | Val MEE: 9.8203\n",
      "Epoch 060 | Train Loss: 137.0988 | Train MEE: 10.5228 | Val Loss: 125.9928 | Val MEE: 9.8012\n",
      "Epoch 061 | Train Loss: 135.0651 | Train MEE: 10.4331 | Val Loss: 121.1298 | Val MEE: 9.6780\n",
      "Epoch 062 | Train Loss: 135.1014 | Train MEE: 10.4459 | Val Loss: 128.6792 | Val MEE: 9.8555\n",
      "Epoch 063 | Train Loss: 134.6395 | Train MEE: 10.4116 | Val Loss: 117.7508 | Val MEE: 9.5178\n",
      "Epoch 064 | Train Loss: 133.8384 | Train MEE: 10.4057 | Val Loss: 118.8356 | Val MEE: 9.5645\n",
      "Epoch 065 | Train Loss: 132.5829 | Train MEE: 10.3572 | Val Loss: 127.3241 | Val MEE: 9.8398\n",
      "Epoch 066 | Train Loss: 133.4833 | Train MEE: 10.3508 | Val Loss: 136.5271 | Val MEE: 10.0581\n",
      "Epoch 067 | Train Loss: 133.1138 | Train MEE: 10.3299 | Val Loss: 119.3423 | Val MEE: 9.5615\n",
      "Epoch 068 | Train Loss: 130.2435 | Train MEE: 10.2400 | Val Loss: 122.4661 | Val MEE: 9.6706\n",
      "Epoch 069 | Train Loss: 130.1871 | Train MEE: 10.2175 | Val Loss: 117.7070 | Val MEE: 9.4755\n",
      "Epoch 070 | Train Loss: 132.3817 | Train MEE: 10.3199 | Val Loss: 127.4388 | Val MEE: 9.8645\n",
      "Epoch 071 | Train Loss: 132.0180 | Train MEE: 10.3063 | Val Loss: 133.5036 | Val MEE: 9.9984\n",
      "Epoch 072 | Train Loss: 130.5985 | Train MEE: 10.1849 | Val Loss: 123.5219 | Val MEE: 9.5833\n",
      "Epoch 073 | Train Loss: 130.6141 | Train MEE: 10.1519 | Val Loss: 135.5524 | Val MEE: 10.0367\n",
      "Epoch 074 | Train Loss: 129.6728 | Train MEE: 10.2009 | Val Loss: 122.9994 | Val MEE: 9.6571\n",
      "Epoch 075 | Train Loss: 135.5019 | Train MEE: 10.3757 | Val Loss: 123.6471 | Val MEE: 9.6314\n",
      "Epoch 076 | Train Loss: 133.0183 | Train MEE: 10.3426 | Val Loss: 122.5160 | Val MEE: 9.6820\n",
      "Epoch 077 | Train Loss: 133.4315 | Train MEE: 10.2837 | Val Loss: 130.5092 | Val MEE: 9.8599\n",
      "Epoch 078 | Train Loss: 128.9951 | Train MEE: 10.1096 | Val Loss: 123.2276 | Val MEE: 9.7124\n",
      "Epoch 079 | Train Loss: 125.5821 | Train MEE: 10.0220 | Val Loss: 128.1487 | Val MEE: 9.7770\n",
      "Epoch 080 | Train Loss: 125.0901 | Train MEE: 9.9651 | Val Loss: 119.5680 | Val MEE: 9.5246\n",
      "Epoch 081 | Train Loss: 124.2004 | Train MEE: 9.9241 | Val Loss: 126.5811 | Val MEE: 9.7506\n",
      "Epoch 082 | Train Loss: 133.3251 | Train MEE: 10.2357 | Val Loss: 136.0295 | Val MEE: 10.0280\n",
      "Epoch 083 | Train Loss: 130.3844 | Train MEE: 10.1552 | Val Loss: 117.7240 | Val MEE: 9.3909\n",
      "Epoch 084 | Train Loss: 125.6809 | Train MEE: 9.9803 | Val Loss: 123.9556 | Val MEE: 9.7044\n",
      "Epoch 085 | Train Loss: 124.2702 | Train MEE: 9.9404 | Val Loss: 122.2088 | Val MEE: 9.5748\n",
      "Epoch 086 | Train Loss: 127.2237 | Train MEE: 10.0417 | Val Loss: 135.6124 | Val MEE: 9.9772\n",
      "Epoch 087 | Train Loss: 122.1098 | Train MEE: 9.8689 | Val Loss: 119.2446 | Val MEE: 9.5334\n",
      "Epoch 088 | Train Loss: 123.2878 | Train MEE: 9.8994 | Val Loss: 127.5526 | Val MEE: 9.7600\n",
      "Epoch 089 | Train Loss: 125.2961 | Train MEE: 9.9772 | Val Loss: 121.2803 | Val MEE: 9.6060\n",
      "Epoch 090 | Train Loss: 128.4071 | Train MEE: 9.9720 | Val Loss: 120.1324 | Val MEE: 9.5320\n",
      "Epoch 091 | Train Loss: 122.5607 | Train MEE: 9.8710 | Val Loss: 124.1280 | Val MEE: 9.6212\n",
      "Epoch 092 | Train Loss: 119.1947 | Train MEE: 9.7491 | Val Loss: 121.3492 | Val MEE: 9.5959\n",
      "Epoch 093 | Train Loss: 119.1735 | Train MEE: 9.6944 | Val Loss: 117.6658 | Val MEE: 9.4039\n",
      "Epoch 094 | Train Loss: 118.7293 | Train MEE: 9.6743 | Val Loss: 126.3474 | Val MEE: 9.6746\n",
      "Epoch 095 | Train Loss: 120.5570 | Train MEE: 9.7557 | Val Loss: 125.1847 | Val MEE: 9.6789\n",
      "Epoch 096 | Train Loss: 120.3954 | Train MEE: 9.7285 | Val Loss: 124.3275 | Val MEE: 9.5182\n",
      "Epoch 097 | Train Loss: 124.0758 | Train MEE: 9.8750 | Val Loss: 131.8334 | Val MEE: 9.8455\n",
      "Epoch 098 | Train Loss: 121.7999 | Train MEE: 9.7664 | Val Loss: 134.3706 | Val MEE: 9.9789\n",
      "Epoch 099 | Train Loss: 122.7179 | Train MEE: 9.7730 | Val Loss: 136.0630 | Val MEE: 10.0198\n",
      "Epoch 100 | Train Loss: 121.1686 | Train MEE: 9.7630 | Val Loss: 117.7086 | Val MEE: 9.3573\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    LinearLayer(12, 32),\n",
    "    ReLU(),\n",
    "    LinearLayer(32, 32),\n",
    "    ReLU(),\n",
    "    LinearLayer(32, 4),\n",
    ")\n",
    "loss_fn = MSELoss()\n",
    "optimizer = AdamWOptimizer(model, learning_rate=0.01, weight_decay=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Train\n",
    "    train_total_n = 0\n",
    "    train_losses = 0.0\n",
    "    train_errors = 0.0\n",
    "    for x_batch, y_batch in XTr_dl:\n",
    "        y_pred = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(y_pred, y_batch)\n",
    "        grad_loss = loss_fn.backward()\n",
    "        model.backward(grad_loss)\n",
    "        optimizer.step()\n",
    "        train_total_n += y_batch.shape[0]\n",
    "        train_losses += loss * y_batch.shape[0]\n",
    "        result = np.sqrt(np.mean((y_pred - y_batch) ** 2, axis=1))\n",
    "        train_errors += np.sum(result)\n",
    "\n",
    "    # Validate\n",
    "    val_total_n = 0\n",
    "    val_losses = 0.0\n",
    "    val_errors = 0.0\n",
    "    for x_batch, y_batch in XVl_dl:\n",
    "        y_pred = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(y_pred, y_batch)\n",
    "        val_total_n += y_batch.shape[0]\n",
    "        val_losses += loss * y_batch.shape[0]\n",
    "        result = np.sqrt(np.mean((y_pred - y_batch) ** 2, axis=1))\n",
    "        val_errors += np.sum(result)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d} | Train Loss: {train_losses / train_total_n:.4f} | Train MEE: {train_errors / train_total_n:.4f} | Val Loss: {val_losses / val_total_n:.4f} | Val MEE: {val_errors / val_total_n:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "5758e41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 118.4052 | Test MEE: 9.6595\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_total_n = 0\n",
    "test_losses = 0.0\n",
    "test_errors = 0.0\n",
    "for x_batch, y_batch in XT_dl:\n",
    "    y_pred = model.forward(x_batch)\n",
    "    loss = loss_fn.forward(y_pred, y_batch)\n",
    "    test_total_n += y_batch.shape[0]\n",
    "    test_losses += loss * y_batch.shape[0]\n",
    "    result = np.sqrt(np.mean((y_pred - y_batch) ** 2, axis=1))\n",
    "    test_errors += np.sum(result)\n",
    "\n",
    "print(f\"Test Loss: {test_losses / test_total_n:.4f} | Test MEE: {test_errors / test_total_n:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9419711",
   "metadata": {},
   "source": [
    "Because we did not scale the target variables, the MEE value is meaningless. We calculate what an blind model that would just output the standard deviation of the training data would end up having as MEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "1716c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blind model MEE: 19.545207605331843\n"
     ]
    }
   ],
   "source": [
    "std_targets = np.std(YTr, axis=0)\n",
    "mee = np.sqrt(np.mean(std_targets ** 2))\n",
    "print(f\"Blind model MEE: {mee}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52d571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
